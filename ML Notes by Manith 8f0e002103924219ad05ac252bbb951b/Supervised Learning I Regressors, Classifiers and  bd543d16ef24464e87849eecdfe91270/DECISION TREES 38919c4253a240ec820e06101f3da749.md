# DECISION TREES

## **What is a Decision Tree?**

Decision trees are machine learning models that try to find patterns in the features of data points. Take a look at the tree on this page. This tree tries to predict whether a student will get an A on their next test.

By asking questions like “What is the student’s average grade in the class” the decision tree tries to get a better understanding of their chances on the next test.

In order to make a classification, this classifier needs a data point with four features:

- The student’s average grade in the class.
- The number of hours the student plans on studying for the test.
- The number of hours the student plans on sleeping the night before the test.
- Whether or not the student plans on cheating.

For example, let’s say that somebody has a “B” average in the class, studied for more than 3 hours, slept less than 5 hours before the test, and doesn’t plan to cheat. If we start at the top of the tree and take the correct path based on that data, we’ll arrive at a *leaf node* that predicts the person will *not* get an A on the next test.

![Untitled](DECISION%20TREES%2038919c4253a240ec820e06101f3da749/Untitled.png)

![Untitled](DECISION%20TREES%2038919c4253a240ec820e06101f3da749/Untitled%201.png)

## **What is a Decision Tree? (Contd.)**

If we’re given this magic tree, it seems relatively easy to make classifications. But how do these trees get created in the first place? Decision trees are supervised machine learning models, which means that they’re created from a training set of labeled data. Creating the tree is where the *learning* in machine learning happens.

Take a look at the gif on this page. We begin with every point in the training set at the top of the tree. These training points have labels — the red points represent students that didn’t get an A on a test and the green points represent students that did get an A on a test.

We then decide to split the data into smaller groups based on a feature. For example, that feature could be something like their average grade in the class. Students with an A average would go into one set, students with a B average would go into another subset, and so on.

Once we have these subsets, we repeat the process — we split the data in each subset again on a different feature. Eventually, we reach a point where we decide to stop splitting the data into smaller groups. We’ve reached a leaf of the tree. We can now count up the labels of the data in that leaf. If an unlabeled point reaches that leaf, it will be classified as the majority label.

We can now make a tree, but how did we know which features to split the data set with? After all, if we started by splitting the data based on the number of hours they slept the night before the test, we’d end up with a very different tree that would produce very different results. How do we know which tree is best? We’ll tackle this question soon!

![tree_gif.gif](DECISION%20TREES%2038919c4253a240ec820e06101f3da749/tree_gif.gif)

## **Implementing a Decision Tree**

To answer the questions posed in the previous exercise, we’re going to do things a bit differently in this lesson and work “backwards” (!!!): we’re going to first fit a decision tree to a dataset and visualize this tree using `scikit-learn`. We’re then going to systematically unpack the following: how to interpret the tree visualization, how `scikit-learn`‘s implementation works, what is gini impurity, what are parameters and hyper-parameters of the decision tree model, etc.

We’re going to use a dataset about cars with six features:

- The price of the car, `buying`, which can be “vhigh”, “high”, “med”, or “low”.
- The cost of maintaining the car, `maint`, which can be “vhigh”, “high”, “med”, or “low”.
- The number of doors, `doors`, which can be “2”, “3”, “4”, “5more”.
- The number of people the car can hold, `persons`, which can be “2”, “4”, or “more”.
- The size of the trunk, `lugboot`, which can be “small”, “med”, or “big”.
- The safety rating of the car, `safety`, which can be “low”, “med”, or “high”.

The question we will be trying to answer using decision trees is: when considering buying a car, what factors go into making that decision?

```python
import pandas as pd
import numpy as np
import codecademylib3
import matplotlib.pyplot as plt

#Import models from scikit learn module:
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
from sklearn import tree

#Loading the dataset
df = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/car/car.data', names=['buying', 'maint', 'doors', 'persons', 'lug_boot', 'safety', 'accep'])

#1a. Take a look at the dataset
print(df.head())

# 1b. Setting the target and predictor variables
df['accep'] = ~(df['accep']=='unacc') #1 is acceptable, 0 if not acceptable
X = pd.get_dummies(df.iloc[:,0:6])
y = df['accep']

# 1c. Examine the new features
print(X.columns)
print(len(X.columns))

# 2a. Performing the train-test split
x_train, x_test, y_train, y_test = train_test_split(X,y, random_state=0, test_size=0.2)

# 2b.Fitting the decision tree classifier
dt = DecisionTreeClassifier(max_depth=3, ccp_alpha=0.01,criterion='gini')
dt.fit(x_train, y_train)

# 3.Plotting the Tree
plt.figure(figsize=(20,12))
tree.plot_tree(dt, feature_names = x_train.columns, max_depth=5, class_names = ['unacc', 'acc'], label='all', filled=True)
plt.tight_layout()
plt.show()
```

![Untitled](DECISION%20TREES%2038919c4253a240ec820e06101f3da749/Untitled%202.png)

## **Interpreting a Decision Tree**

We’re now going to examine the decision tree we built for the car dataset. The image to the right of the code editor is the exact plot you created in the previous exercise. Two important concepts to note here are the following:

1. The root node is identified as the top of the tree. This is notated already with the number of samples and the numbers in each class (i.e. unacceptable vs. acceptable) that was used to build the tree.
2. Splits occur with True to the left, False to the right. Note the right split is a `leaf node` i.e., there are no more branches. Any decision ending here results in the majority class. (The majority class here is `unacc`.)

(Note that there is a term called `gini` in each of the boxes that is immensely important for how the split is done - we will explore this in the following exercise!)

To interpret the tree, it’s useful to keep in mind that the variables we’re looking at are categorical variables that correspond to:

- `buying`: The price of the car which can be “vhigh”, “high”, “med”, or “low”.
- `maint`: The cost of maintaining the car which can be “vhigh”, “high”, “med”, or “low”.
- `doors`: The number of doors which can be “2”, “3”, “4”, “5more”.
- `persons`: The number of people the car can hold which can be “2”, “4”, or “more”.
- `lugboot`: The size of the trunk which can be “small”, “med”, or “big”.
- `safety`: The safety rating of the car which can be “low”, “med”, or “high”.

![Untitled](DECISION%20TREES%2038919c4253a240ec820e06101f3da749/Untitled%203.png)

## **Gini Impurity**

Consider the two trees below. Which tree would be more useful as a model that tries to predict whether someone would get an A in a class?

![https://content.codecademy.com/programs/data-science-path/decision-trees/comparison_1.svg](https://content.codecademy.com/programs/data-science-path/decision-trees/comparison_1.svg)

![https://content.codecademy.com/programs/data-science-path/decision-trees/comparison_2.svg](https://content.codecademy.com/programs/data-science-path/decision-trees/comparison_2.svg)

Let’s say you use the top tree. You’ll end up at a leaf node where the label is up for debate. The training data has labels from both classes! If you use the bottom tree, you’ll end up at a leaf where there’s only one type of label. There’s no debate at all! We’d be much more confident about our classification if we used the bottom tree.

This idea can be quantified by calculating the *Gini impurity* of a set of data points. For two classes (1 and 2) with probabilites `p_1` and `p_2` respectively, the Gini impurity is:

![Untitled](DECISION%20TREES%2038919c4253a240ec820e06101f3da749/Untitled%204.png)

![https://static-assets.codecademy.com/skillpaths/ml-fundamentals/decision_trees/gini_impurity_graph.png](https://static-assets.codecademy.com/skillpaths/ml-fundamentals/decision_trees/gini_impurity_graph.png)

The goal of a decision tree model is to separate the classes the best possible, i.e. minimize the impurity (or maximize the purity). Notice that if

p_1 is 0 or 1, the Gini impurity is 0 , which means there is only one class so there is perfect separation. From the graph, the Gini impurity is maximum at p_1=0.5 , which means the two classes are equally balanced, so this is perfectly impure!

In general, the Gini impurity for C classes is defined as:

![Untitled](DECISION%20TREES%2038919c4253a240ec820e06101f3da749/Untitled%205.png)

## **Information Gain**

We know that we want to end up with leaves with a low Gini Impurity, but we still need to figure out which features to split on in order to achieve this. To answer this question, we can calculate the *information gain* of splitting the data on a certain feature. Information gain measures the difference in the impurity of the data before and after the split.

For example, let’s start with the root node of our car acceptability tree:

![https://static-assets.codecademy.com/skillpaths/ml-fundamentals/decision_trees/dtree_info_gain.png](https://static-assets.codecademy.com/skillpaths/ml-fundamentals/decision_trees/dtree_info_gain.png)

The initial Gini impurity (which we confirmed previously) is 0.418. The first split occurs based on the feature `safety_low<=0.5`, and as this is a dummy variable with values 0 and 1, this split is pushing not low safety cars to the left (912 samples) and low safety cars to the right (470 samples). Before we discuss how we decided to split on this feature, let’s calculate the information gain.

The new Gini impurities for these two split nodes is 0.495 and 0 (which is a pure leaf node!). All together, the now weighted Gini impurity after the split is:

![Untitled](DECISION%20TREES%2038919c4253a240ec820e06101f3da749/Untitled%206.png)

Not bad! (Remember we *want* our Gini impurity to be lower!) This is lower than our initial Gini impurity, so by splitting the data in that way, we’ve gained some information about how the data is structured — the datasets after the split are purer than they were before the split.

Then the information gain (or reduction in impurity after the split) is

![Untitled](DECISION%20TREES%2038919c4253a240ec820e06101f3da749/Untitled%207.png)

The higher the information gain the better — if information gain is 0, then splitting the data on that feature was useless!

```python
#1. Information gain at a pure node (i.e., node with no more branches!)
r = 0.5 #ratio of new split, could be anything
gini_pure_node = 0
gini_info_gain = r*gini_pure_node  + (1-r)*gini_pure_node 
print(f'Gini information gain pure node split safety_low >= .5 : {gini_info_gain}')

#2. Information gain at the 'persons_2' split
r_persons_2 = 604/912 #read ratio of the split from the tree!
gini_left_split = 0.434
gini_right_split = 0
gini_info_gain_persons_2 = r_persons_2*gini_left_split + r_persons_2*gini_right_split
print(f'Gini information gain node persons_2 : {gini_info_gain_persons_2}')
```

## **How a Decision Tree is Built (Feature Split)**

Now that we know about Gini impurity and information gain, we can understand how the decision tree was built. We’ve been using an already built tree, where the root node is split on `safety_low<=0.5`, which is true when the value is 0, the left split (vehicles WITHOUT low safety), and false when the value is 1, the right split (vehicles WITH low safety). We saw in the previous exercise the information gain for this feature was

![Untitled](DECISION%20TREES%2038919c4253a240ec820e06101f3da749/Untitled%208.png)

Now, let’s compare that with a different feature we could have split on first, `persons_2`. In this case, the left branch will have a Gini impurity of

![Untitled](DECISION%20TREES%2038919c4253a240ec820e06101f3da749/Untitled%209.png)

while the right split has a Gini impurity of 0. (Don’t worry about the numbers - you will be able to verify this presently!)

Then the weighted impurity is

![Untitled](DECISION%20TREES%2038919c4253a240ec820e06101f3da749/Untitled%2010.png)

The information gain then is

![Untitled](DECISION%20TREES%2038919c4253a240ec820e06101f3da749/Untitled%2011.png)

Since this is less than the information gain from `safety_low`, this is not a better split and so it is not used. We would continue this process with ALL features, but, spoiler, `safety_low` gives the largest information gain, and so was chosen first. To verify that this is indeed the case, we have two functions pre-written in the workspace, `gini` and `info_gain` that calculate Gini impurity and information gain at any node.

```python
## The usual libraries, loading the dataset and performing the train-test split
import pandas as pd
from sklearn.model_selection import train_test_split

df = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/car/car.data', names=['buying', 'maint', 'doors', 'persons', 'lug_boot', 'safety', 'accep'])
df['accep'] = ~(df['accep']=='unacc') #1 is acceptable, 0 if not acceptable
X = pd.get_dummies(df.iloc[:,0:6])
y = df['accep']

x_train, x_test, y_train, y_test = train_test_split(X,y, random_state=0, test_size=0.2)

## Functions to calculate gini impurity and information gain

def gini(data):
    """calculate the Gini Impurity
    """
    data = pd.Series(data)
    return 1 - sum(data.value_counts(normalize=True)**2)
   
def info_gain(left, right, current_impurity):
    """Information Gain associated with creating a node/split data.
    Input: left, right are data in left branch, right banch, respectively
    current_impurity is the data impurity before splitting into left, right branches
    """
    # weight for gini score of the left branch
    w = float(len(left)) / (len(left) + len(right))
    return current_impurity - w * gini(left) - (1 - w) * gini(right)

## 1. Calculate gini and info gain for a root node split at safety_low<=0.5
y_train_sub = y_train[x_train['safety_low']==0]
x_train_sub = x_train[x_train['safety_low']==0]

gi = gini(y_train_sub)
print(f'Gini impurity at root: {gi}')

## 2. Information gain when using feature `persons_2`
left = y_train[x_train['persons_2']==0]
right = y_train[x_train['persons_2']==1]

print(f'Information gain for persons_2: {info_gain(left, right, gi)}')

## 3. Which feature split maximizes information gain?

info_gain_list = []
for i in x_train.columns:
    left = y_train_sub[x_train_sub[i]==0]
    right = y_train_sub[x_train_sub[i]==1]
    info_gain_list.append([i, info_gain(left, right, gi)])

info_gain_table = pd.DataFrame(info_gain_list).sort_values(1,ascending=False)
print(f'Greatest impurity gain at:{info_gain_table.iloc[0,:]}')
print(info_gain_table)
```

## **How a Decision Tree is Built (Recursion)**

Now that we can find the best feature to split the dataset, we can repeat this process again and again to create the full tree. This is a recursive algorithm! We start with every data point from the training set, find the best feature to split the data, split the data based on that feature, and then recursively repeat the process again on each subset that was created from the split.

We’ll stop the recursion when we can no longer find a feature that results in any information gain. In other words, we want to create a leaf of the tree when we can’t find a way to split the data that makes purer subsets.

![Untitled](DECISION%20TREES%2038919c4253a240ec820e06101f3da749/Untitled%2012.png)

```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
from sklearn import tree

def gini(data):
    """calculate the Gini Impurity
    """
    data = pd.Series(data)
    return 1 - sum(data.value_counts(normalize=True)**2)
   
def info_gain(left, right, current_impurity):
    """Information Gain associated with creating a node/split data.
    Input: left, right are data in left branch, right banch, respectively
    current_impurity is the data impurity before splitting into left, right branches
    """
    # weight for gini score of the left branch
    w = float(len(left)) / (len(left) + len(right))
    return current_impurity - w * gini(left) - (1 - w) * gini(right)

df = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/car/car.data', names=['buying', 'maint', 'doors', 'persons', 'lug_boot', 'safety', 'accep'])
df['accep'] = ~(df['accep']=='unacc') #1 is acceptable, 0 if not acceptable
X = pd.get_dummies(df.iloc[:,0:6])
y = df['accep']

x_train, x_test, y_train, y_test = train_test_split(X,y, random_state=0, test_size=0.2)

y_train_sub = y_train[x_train['safety_low']==0]
x_train_sub = x_train[x_train['safety_low']==0]

gi = gini(y_train_sub)
print(f'Gini impurity at root: {gi}')

info_gain_list = []
for i in x_train.columns:
    left = y_train_sub[x_train_sub[i]==0]
    right = y_train_sub[x_train_sub[i]==1]
    info_gain_list.append([i, info_gain(left, right, gi)])

info_gain_table = pd.DataFrame(info_gain_list).sort_values(1,ascending=False)
print(f'Greatest impurity gain at:{info_gain_table.iloc[0,:]}')
```

## **Train and Predict using `scikit-learn`**

Now we’ll finally build a decision tree ourselves! We will use `scikit-learn`‘s tree module to create, train, predict, and visualize a decision tree classifier. The syntax is the same as other models in `scikit-learn`, so it should look very familiar. First, an instance of the model class is instantiated with `DecisionTreeClassifier()`. To use non-default hyperparameter values, you can pass them at this stage, such as `DecisionTreeClassifier(max_depth=5)`.

Then `.fit()` takes a list of data points followed by a list of the labels associated with that data and builds the decision tree model.

Finally, once we’ve made our tree, we can use it to classify new data points. The `.predict()` method takes an array of data points and will return an array of classifications for those data points. `predict_proba()` can also be used to return class probabilities instead. Last, `.score()` can be used to generate the accuracy score for a new set of data and labels.

As with other sklearn models, only numeric data can be used (categorical variables and nulls must be handled prior to model fitting).

```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
from sklearn import tree
from sklearn.metrics import accuracy_score

df = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/car/car.data', names=['buying', 'maint', 'doors', 'persons', 'lug_boot', 'safety', 'accep'])
df['accep'] = ~(df['accep']=='unacc') #1 is acceptable, 0 if not acceptable
X = pd.get_dummies(df.iloc[:,0:6])
y = df['accep']
x_train, x_test, y_train, y_test = train_test_split(X,y, random_state=0, test_size=0.2)

## 1. Create a decision tree and print the parameters
dtree = DecisionTreeClassifier()
print(f'Decision Tree parameters: {dtree.get_params()}')

## 2. Fit decision tree on training set and print the depth of the tree
dtree.fit(x_train,y_train)

print(f'Decision tree depth: {dtree.get_depth()}')

## 3. Predict on test data and accuracy of model on test set
y_pred = dtree.predict(x_test)

print(f'Test set accuracy: {dtree.score(x_test,y_test)}') # or accuracy_score(y_test, y_pred)
```

output

```python
Decision Tree parameters: {'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': None, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_impurity_split': None, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'presort': 'deprecated', 'random_state': None, 'splitter': 'best'}
Decision tree depth: 11
Test set accuracy: 0.9884393063583815
```

## **Visualizing Decision Trees**

Great, we built a decision tree using `scikit-learn` and predicted new values with it! But what does the tree look like? What features are used to split? Two methods using only `scikit-learn`/`matplotlib` can help visualize the tree, the first using `tree_plot`, the second listing the rules as text. There are other libraries available with more advanced visualization (`graphviz` and `dtreeviz`, for example, but may require additional installation and won’t be covered here).

```python
import codecademylib3
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
from sklearn import tree

## Loading the data and setting target and predictor variables
df = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/car/car.data', names=['buying', 'maint', 'doors', 'persons', 'lug_boot', 'safety', 'accep'])
df['accep'] = ~(df['accep']=='unacc') #1 is acceptable, 0 if not acceptable
X = pd.get_dummies(df.iloc[:,0:6])
y = df['accep']

## Train-test split and fitting the tree
x_train, x_test, y_train, y_test = train_test_split(X,y, random_state=0, test_size=0.3)
dtree = DecisionTreeClassifier(max_depth=3)
dtree.fit(x_train, y_train)

## Visualizing the tree
plt.figure(figsize=(27,12))
tree.plot_tree(dtree)
plt.tight_layout()
plt.show()

## Text-based visualization of the tree (View this in the Output terminal!)
print(tree.export_text(dtree))
```

![Untitled](DECISION%20TREES%2038919c4253a240ec820e06101f3da749/Untitled%2013.png)

```python
|--- feature_19 <= 0.50
|   |--- feature_12 <= 0.50
|   |   |--- feature_7 <= 0.50
|   |   |   |--- class: True
|   |   |--- feature_7 >  0.50
|   |   |   |--- class: False
|   |--- feature_12 >  0.50
|   |   |--- class: False
|--- feature_19 >  0.50
|   |--- class: False
```

## **Advantages and Disadvantages**

As we have seen already, decision trees are easy to understand, fully explainable, and have a natural way to visualize the decision making process. In addition, often little modification needs to be made to the data prior to modeling (such as scaling, normalization, removing outliers) and decision trees are relatively quick to train and predict. However, now let’s talk about some of their limitations.

One problem with the way we’re currently making our decision trees is that our trees aren’t always *globally optimal*. This means that there might be a better tree out there somewhere that produces better results. But wait, why did we go through all that work of finding information gain if it’s not producing the best possible tree?

Our current strategy of creating trees is *greedy*. We assume that the best way to create a tree is to find the feature that will result in the largest information gain *right now* and split on that feature. We never consider the ramifications of that split further down the tree. It’s possible that if we split on a suboptimal feature right now, we would find even better splits later on. Unfortunately, finding a globally optimal tree is an extremely difficult task, and finding a tree using our greedy approach is a reasonable substitute.

Another problem with our trees is that they are prone to *overfit* the data. This means that the structure of the tree is too dependent on the training data and may not generalize well to new data. In general, larger trees tend to overfit the data more. As the tree gets bigger, it becomes more tuned to the training data and it loses a more generalized understanding of the real world data.

```python
import pandas as pd
import numpy as np
import codecademylib3
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
from sklearn import tree

df = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/car/car.data', names=['buying', 'maint', 'doors', 'persons', 'lug_boot', 'safety', 'accep'])
df['accep'] = ~(df['accep']=='unacc') #1 is acceptable, 0 if not acceptable
X = pd.get_dummies(df.iloc[:,0:6])
y = df['accep']

x_train, x_test, y_train, y_test = train_test_split(X,y, random_state=0, test_size=0.3)

## 1. Two decision trees
dtree1 = DecisionTreeClassifier()
dtree2 = DecisionTreeClassifier(max_depth = 7)

## Fit first decision tree
dtree1.fit(x_train, y_train)
dtree1_depth = dtree1.get_depth()
print(f'First Decision Tree depth: {dtree1_depth}')

## Fit second decision tree
dtree2.fit(x_train, y_train)
dtree2_depth = dtree2.get_depth()
print(f'Second Decision Tree depth: {dtree2_depth}')

## 2. Calculate accuracy scores on test data for both trees

dtree1_score = dtree1.score(x_test, y_test)
print(f'Test set accuracy tree no max depth: {dtree1_score}')# or accuracy_score(y_test, y_pred)

dtree2_score = dtree2.score(x_test, y_test)
print(f'Test set accuracy tree max depth 7: {dtree2_score}')# or accuracy_score(y_test, y_pred)
```

output

```python
First Decision Tree depth: 11
Second Decision Tree depth: 7
Test set accuracy tree no max depth: 0.9788053949903661
Test set accuracy tree max depth 7: 0.9730250481695568
```

# **Normalization**

**This article describes why normalization is necessary. It also demonstrates the pros and cons of min-max normalization and z-score normalization.**

### **Why Normalize?**

Many machine learning algorithms attempt to find trends in the data by comparing features of data points. However, there is an issue when the features are on drastically different scales.

For example, consider a dataset of houses. Two potential features might be the number of rooms in the house, and the total age of the house in years. A machine learning algorithm could try to predict which house would be best for you. However, when the algorithm compares data points, the feature with the larger scale will completely dominate the other. Take a look at the image below:

![https://static-assets.codecademy.com/Paths/data-science-career-path/MachineLearning/unnormalized.PNG](https://static-assets.codecademy.com/Paths/data-science-career-path/MachineLearning/unnormalized.PNG)

When the data looks squished like that, we know we have a problem. The machine learning algorithm should realize that there is a huge difference between a house with 2 rooms and a house with 20 rooms. But right now, because two houses can be 100 years apart, the difference in the number of rooms contributes less to the overall difference.

As a more extreme example, imagine what the graph would look like if the x-axis was the cost of the house. The data would look even more squished; the difference in the number of rooms would be even less relevant because the cost of two houses could have a difference of thousands of dollars.

The goal of normalization is to make every datapoint have the same scale so each feature is equally important. The image below shows the same house data normalized using min-max normalization.

![https://static-assets.codecademy.com/Paths/data-science-career-path/MachineLearning/normalized.png](https://static-assets.codecademy.com/Paths/data-science-career-path/MachineLearning/normalized.png)

### **Min-Max Normalization**

Min-max normalization is one of the most common ways to normalize data. For every feature, the minimum value of that feature gets transformed into a 0, the maximum value gets transformed into a 1, and every other value gets transformed into a decimal between 0 and 1.

For example, if the minimum value of a feature was 20, and the maximum value was 40, then 30 would be transformed to about 0.5 since it is halfway between 20 and 40. The formula is as follows:

![Untitled](DECISION%20TREES%2038919c4253a240ec820e06101f3da749/Untitled%2014.png)

Min-max normalization has one fairly significant downside: it does not handle outliers very well. For example, if you have 99 values between 0 and 40, and one value is 100, then the 99 values will all be transformed to a value between 0 and 0.4. That data is just as squished as before! Take a look at the image below to see an example of this.

![https://static-assets.codecademy.com/Paths/data-science-career-path/MachineLearning/outlier.png](https://static-assets.codecademy.com/Paths/data-science-career-path/MachineLearning/outlier.png)

Normalizing fixed the squishing problem on the y-axis, but the x-axis is still problematic. Now if we were to compare these points, the y-axis would dominate; the y-axis can differ by 1, but the x-axis can only differ by 0.4.

### **Z-Score Normalization**

Z-score normalization is a strategy of normalizing data that avoids this outlier issue. The formula for Z-score normalization is below:

![Untitled](DECISION%20TREES%2038919c4253a240ec820e06101f3da749/Untitled%2015.png)

Here, μ is the mean value of the feature and σ is the standard deviation of the feature. If a value is exactly equal to the mean of all the values of the feature, it will be normalized to 0. If it is below the mean, it will be a negative number, and if it is above the mean it will be a positive number. The size of those negative and positive numbers is determined by the standard deviation of the original feature. If the unnormalized data had a large standard deviation, the normalized values will be closer to 0.

Take a look at the graph below. This is the same data as before, but this time we’re using z-score normalization.

![https://static-assets.codecademy.com/Paths/data-science-career-path/MachineLearning/z-score.png](https://static-assets.codecademy.com/Paths/data-science-career-path/MachineLearning/z-score.png)

While the data still *looks* squished, notice that the points are now on roughly the same scale for both features — almost all points are between -2 and 2 on both the x-axis and y-axis. The only potential downside is that the features aren’t on the *exact* same scale.

With min-max normalization, we were guaranteed to reshape both of our features to be between 0 and 1. Using z-score normalization, the x-axis now has a range from about -1.5 to 1.5 while the y-axis has a range from about -2 to 2. This is certainly better than before; the x-axis, which previously had a range of 0 to 40, is no longer dominating the y-axis.

### **Review**

Normalizing your data is an essential part of machine learning. You might have an amazing dataset with many great features, but if you forget to normalize, one of those features might completely dominate the others. It’s like you’re throwing away almost all of your information! Normalizing solves this problem. In this article, you learned the following techniques to normalize:

- **Min-max normalization**: Guarantees all features will have the exact same scale but does not handle outliers well.
- **Z-score normalization**: Handles outliers, but does not produce normalized data with the *exact* same scale.