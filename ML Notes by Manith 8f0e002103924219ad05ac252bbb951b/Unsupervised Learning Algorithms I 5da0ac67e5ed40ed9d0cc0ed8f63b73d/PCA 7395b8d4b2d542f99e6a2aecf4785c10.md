# PCA

# **What is PCA?**

**Learn how, when, and why to use Principal Component Analysis (PCA) as part of the Machine Learning lifecycle.**

### **Introduction to PCA**

In the world of Machine Learning, any model that we implement will be more valuable when the features are engineered to suit the question we’re trying to answer. With many datasets, we can simply include all available features, which gives us the full picture about our observations. For example, it’s straightforward to see a correlation between height and weight for a patient dataset. Some datasets, however, have very large numbers of features. If our example patient dataset expanded to include 20 different features, how would we visualize and correlate this data? When it comes time to actually process the data and train the model, we often hit computational or complexity limits. How do we leverage correlations within the data to make fewer, better features without losing the information included in the dataset?

Situations like this are a great use case for implementing Principal Component Analysis. PCA is a technique where we can reduce the number of features in a dataset without losing any of the information we have. Sounds pretty great right? This article will cover various aspects of PCA, so let’s dive in.

### **Laying the groundwork for PCA**

Before we dive into the specifics of PCA, we need to understand the importance of information. In particular, we need to understand how variance plays into the level of information in a dataset. For the purposes of this article, we will be looking at a synthetic dataset about local pizza stores. Let’s see what data we have:

```python
import pandas as pd
import matplotlib.pyplot as plt

df = pd.read_csv('pizza.csv')

print(df.columns)
```

Output:

```python
Index(['revenue', 'total_customers', 'amt_flour', 'amt_tomatoes',
       'amt_cheese'],
      dtype='object')
```

Each row of data pertains to an individual store, and gives information about how the store is doing overall with inventory and sales. Suppose we look at just **`revenue`** and **`total_customers`**, and we see the following information:

| revenue | total_customers |
| --- | --- |
| 12345 | 500 |
| 13425 | 500 |
| 10872 | 500 |
| 9561 | 500 |

In this scenario, the value for **`total_customers`** has a value of **`500`** for every row. While every row has a value, and therefore has *data*, this column does not provide a lot of *information*, due to the lack of variance in the values. While we could include this feature in our downstream analytics, it doesn’t provide any additional value, because each row would have the same data.

Now let’s look at what the real data shows us for these two columns:

```python
df[['revenue','total_customers']].head()
```

Output:

```python
    revenue    total_customers
0    9931.860710    615.336682
1    12397.798907    725.440590
2    11983.079340    630.987797
3    13910.984353    746.264763
4    13083.859701    689.060436
```

As we can see, the real dataset has far more variance in these two columns. Since each feature has significant variance, these features provide valuable information about our observations, and should therefore be included in our analysis.

Variance alone is one indicator of the level of information in a dataset, but is not the only factor. To expand on the idea of variance within a dataset, we will look at the Coefficient of Variance, or CV for short. The premise here is that variance must be taken into context with the central tendencies of that dataset. For example, if a dataset has a variance of 5, that will mean very different things if the mean is 2 vs. a dataset with a mean of 100.

Now, let’s actually calculate the Coefficient of Variance for each of our columns.

```python
import numpy as np

#define function to calculate cv
cv = lambda x: np.std(x, ddof=1) / np.mean(x) * 100

print(df.apply(cv))
```

Output:

```python
revenue            10.001034
total_customers     5.138628
amt_flour           9.128946
amt_tomatoes        9.926973
amt_cheese          6.401035
dtype: float64
```

All of the features in this dataset have enough variance where they will be useful in analysis. Since variance is an important factor to PCA, these features will ultimately be ordered by the level of information (i.e. variance) they have. For this dataset, that means, in order of importance, PCA will look at **`revenue`**, **`amt_tomatoes`**, **`amt_flour`**, **`amt_cheese`**, and then **`total_customers`**. While the results of PCA won’t resemble our original features, they will be a mathematical representation of the information contained in the original features, which has value for analytical purposes.

Coding question

The kind of information we have can vary from dataset to dataset, and thus can the Coefficient of Variance. Use what you just learned on a new set of synthetic pizza store data, **`pizza_new.csv`**. Calculate the Coefficients of Variance for each feature in the dataset. Then, create a ranked order Python list for the features in the dataset in terms of information for PCA, from most important to least important.

1

2

3

4

5

6

7

8

9

10

11

12

import numpy as np

import pandas as pd

# Load in new pizza dataset

df = pd.read_csv('pizza_new.csv')

df.head()

# Calculate coefficient of variance for

every feature

# Rank order of importance from highest

to lowest (in a list)

importance_rank = []

Output:

```

```

**Run**

**Check answer**

Run your code to check your answer

### **The Math Behind PCA**

At this point, we need to address how we can actually take information from multiple features and distill it down into a smaller number of features. Let’s dive deeper into each of the steps that lead to PCA.

### **Data Matrix**

First, we need to isolate a *Data Matrix*, another name for a dataset. This data matrix holds all of the features and information that we are interested in. Many datasets will have columns that hold information (i.e. features), and other columns that we want to predict (i.e. labels). Using our previous pizza dataset, we have 5 features in our data matrix.

| revenue | total_customers | amt_flour | amt_tomatoes | amt_cheese |
| --- | --- | --- | --- | --- |
| 9931.860710 | 615.336682 | 37.662830 | 174.102712 | 139.402208 |
| 12397.798907 | 725.440590 | 44.424509 | 239.119556 | 168.425842 |
| 11983.079340 | 630.987797 | 40.259276 | 224.084121 | 146.612426 |
| 13910.984353 | 746.264763 | 43.633485 | 227.096619 | 170.726464 |
| 13083.859701 | 689.060436 | 48.964844 | 221.383478 | 154.786070 |

### **Covariance Matrix**

From here, the next step of PCA is to calculate a *covariance matrix*. Essentially, a covariance matrix is calculating how much a feature changes with changes in every other feature, i.e., we’re looking at the relative variance between any two features. Mathematically, the formula for covariance between two features **`X`** and **`Y`** is:

![Untitled](PCA%207395b8d4b2d542f99e6a2aecf4785c10/Untitled.png)

We will do this equation for the relationship between each of our features, ultimately resulting in a covariance matrix that shows relationships for the entire dataset. Simplifying our example dataset, we could think about our pizza dataset having five individual features with the names **`a`**, **`b`**, **`c`**, **`d`**, and **`e`**. Our ultimate covariance matrix, thus, would end up looking like this:

![Untitled](PCA%207395b8d4b2d542f99e6a2aecf4785c10/Untitled%201.png)

Luckily, with the **`pandas`** package, we can calculate a covariance matrix with the **`.cov()`** method. For our pizza dataset, this results in the following:

```python
    revenue        total_customers    amt_flour    amt_tomatoes    amt_cheese
revenue        1.563517e+06    31853.053820    3713.664277    19980.869509    9152.568482
total_customers    3.185305e+04    1295.096885    105.909335    577.443015    256.641069
amt_flour    3.713664e+03    105.909335    16.894551    66.165738    29.130750
amt_tomatoes    1.998087e+04    577.443015    66.165738    500.715936    162.221734
amt_cheese    9.152568e+03    256.641069    29.130750    162.221734    105.370280
```

One important point to note is that along the primary diagonal (from top-left to bottom-right), we see the same variance values that we calculated for each individual column earlier on.

### **Matrix Factorization, Eigenvalues, and Eigenvectors**

We now have a matrix of variance values for our features. The next step in PCA revolves around *matrix factorization*. Without going into too much detail, our goal with matrix factorization is to find a pair of smaller matrices whose product would equal our covariance matrix. Another way of thinking about it: we want to find a smaller matrix that captures the majority of our information.

An important part of this matrix factorization are *Eigenvectors*. Eigenvectors are vectors (mathematical concepts that have direction and magnitude) that do not change direction when a transformation is applied to them. In the context of data matrices, these eigenvectors give us a direction to “rotate” the dataset in n-dimensional space so we can look at the entire dataset from a simplified perspective. The *eigenvalues* are related to the relative variation described by each principal component.

For a matrix **`A`**, the eigenvectors and eigenvalues are the solutions to the following equation:

![Untitled](PCA%207395b8d4b2d542f99e6a2aecf4785c10/Untitled%202.png)

After some linear algebra, for our covariance matrix, we are looking for the solution to the following matrix, which will be our eigenvectors and eigenvalues.

![Untitled](PCA%207395b8d4b2d542f99e6a2aecf4785c10/Untitled%203.png)

![Untitled](PCA%207395b8d4b2d542f99e6a2aecf4785c10/Untitled%204.png)

### **Principal Components**

All of the underlying math behind PCA results in *principal components*, but what exactly are they? Principal components are a linear combination of all the input features from the original dataset. By using the eigenvectors we calculated earlier, we can “rotate” our dataset features from an n-dimensional space into a 2-dimensional space, which is easier for us to understand and analyze.

To illustrate this point, let’s return to our pizza dataset. We can observe the correlation between our **`revenue`** and **`total_customers`** features.

```python
sns.scatterplot(x='total_customers',
    y='revenue',
    data=df)
```

![https://static-assets.codecademy.com/skillpaths/ml-fundamentals/PCA/revenue-customer-scatterplot.png](https://static-assets.codecademy.com/skillpaths/ml-fundamentals/PCA/revenue-customer-scatterplot.png)

We can see a positive correlation between these two features, and could use that information to guide any analysis we perform. We can also do correlation plots for every combination of features, like so:

```python
sns.pairplot(df)
```

![https://static-assets.codecademy.com/skillpaths/ml-fundamentals/PCA/all-scatterplots.png](https://static-assets.codecademy.com/skillpaths/ml-fundamentals/PCA/all-scatterplots.png)

Each individual combination of features will have its own correlation and variance, both of which provide valuable information about that relationship. When comparing two features at a time, these relationships are more understandable. If we wanted to, however, look at all of the feature relationships and information at once, it would be very difficult to decipher, as we cannot visualize data in a 5-dimensional space.

By using PCA, however, we can reduce the dimensionality of our dataset into a 2-dimensional dataset, allowing for better visualization. Let’s see the result.

```python
from sklearn.decomposition import PCA

pca = PCA(n_components=2)
pca_array = pca.fit_transform(df)
```

Output:

```python
array([[-2572.64663126,   -37.43225524],
       [ -104.21066949,    31.54952593],
       [ -521.0563251 ,   -54.19045713],
       ...,
       [ 1429.58053669,    -5.98229122],
       [-3550.23561932,    23.8935932 ],
       [ -481.85213117,   -34.14891261]])
```

As we can see, by running PCA on our original dataset, we were able to take our 5 features and reduce the dimensions down to 2 principal components. With 2 dimensions, we can now plot the data on a single scatterplot:

```python
sns.scatterplot(pca_array[:,0], pca_array[:,1])
```

![https://static-assets.codecademy.com/skillpaths/ml-fundamentals/PCA/pca_pizza_plot.png](https://static-assets.codecademy.com/skillpaths/ml-fundamentals/PCA/pca_pizza_plot.png)

While it can be difficult to interpret what this new data matrix is showing us, it does hold valuable information that can be used in a variety of contexts. The axes of this chart are the two most impactful principal components as part of our analysis, and were the two that we decided to keep.

Fill in the blank

The below statements outline the overall process of PCA. Fill in the blanks with the correct terms to validate your understanding.

```
For a given dataset, we start by calculating a  for all of our features.

Afterwards, we perform , which will separate out the dataset and give us two results:
  1) , also known as Principal Components, which define the direction, or "rotation", of our new data space
  2) , which determine the magnitude of that new data space
```

- correlation matrix
- eigenvectors
- principal values
- eigenvalues
- principal vectors
- matrix decomposition
- matrix factorization
- covariance matrix

Click or drag and drop to fill in the blank

**Check answer**

### **The How, Where, and Why of PCA**

PCA serves an important role in many different parts of data science and analytics in general, as this process allows us to maximize the amount of information we can extract from data while reducing computational time down the line. We just saw a common use case for PCA with our pizza dataset. We took a higher dimensional dataset (5 dimensions in our case), and reduced it down to 2 dimensions. This two-dimensional dataset can now be an input to a variety of Machine Learning models. For example, we could use this new dataset as part of a forecasting model, or perform linear regression. These techniques would have been much more difficult prior to performing PCA.

PCA is also inherently an unsupervised learning algorithm and can be used to identify clusters in data on its own. Very similar to the popular **`k-means`** algorithms, PCA will look at overall similarities between the different features in a dataset. When we set the number of principal components to keep, we are defining the number of similar “rotations” of our dataset, which will act very much like a cluster of their own. Typically, many practitioners will implement PCA as a precursor to other clustering algorithms to augment the accuracy, but it is an interesting application to do clustering with PCA alone!

Another, very powerful, application of PCA is with image processing. Images hold a vast amount of information in each file, and analyzing this information can have very useful applications. Image classification, for example, uses algorithms to detect the subject of an image, or find a particular object within the image. Overall, it can be very costly to process image data, due to the high dimensionality it has. By applying PCA, however, practitioners can reduce the number of features for the image with minimal information loss and continue their processing.

Below are two images from a Fruit and Vegetable Image dataset. The first image is the original image, and the second is the reconstructed image using the calculated eigenvectors after performing PCA. Note that, in this very quick example, there is minimal information loss, meaning that data could be effectively used for analytics.

![https://static-assets.codecademy.com/skillpaths/ml-fundamentals/PCA/original_pineapple.jpg](https://static-assets.codecademy.com/skillpaths/ml-fundamentals/PCA/original_pineapple.jpg)

![https://static-assets.codecademy.com/skillpaths/ml-fundamentals/PCA/reconstructed_pineapple.png](https://static-assets.codecademy.com/skillpaths/ml-fundamentals/PCA/reconstructed_pineapple.png)

### **Summary**

In this article, you learned the underlying mathematics behind Principal Component Analysis (PCA) and got a bird’s eye view of how, when, and where to implement PCA. Principal Component Analysis is a powerful tool that provides a mechanism to reduce dimensionality and simplify datasets without losing the valuable information they inherently contain. The following image summarizes the different applications of PCA and we are now ready to delve into these in detail!

![https://static-assets.codecademy.com/skillpaths/ml-fundamentals/PCA%20Summary.png](https://static-assets.codecademy.com/skillpaths/ml-fundamentals/PCA%20Summary.png)

**IMPLEMENTING PCA IN PYTHON**

## **Introduction to Implementing PCA**

In this lesson, we will be implementing Principal Component Analysis (PCA) using the Python libraries NumPy and scikit-learn.

The motivation of Principal Component Analysis (PCA) is to find a new set of features that are ordered by the amount of variation (and therefore, information) they contain. We can then select a subset of these PCA features. This leaves us with lower-dimensional data that still retains most of the information contained in the larger dataset.

In this lesson, we will:

- Implement PCA in NumPy step-by-step
- Implement PCA in scikit-learn using only a few lines of code
- Use principal components to train a machine learning model
- Visualize principal components using image data

For the next few exercises, we will use a dataset that describes several types of dry beans separated into seven categories.

We will begin by taking a look at the features that describe different categories of beans.

```python
import pandas as pd
import codecademylib3

# Read the csv data as a DataFrame
df = pd.read_csv('./Dry_Bean.csv')

# Remove null and na values
df.dropna()

# 1. Print the DataFrame head
print(df.head())

# 2. Extract the numerical columns
data_matrix = df.drop(columns='Class')

# Extract the classes
classes = df['Class']

print(data_matrix)
```

## **Implementing PCA in NumPy I**

In this exercise, we will perform PCA using the NumPy method `np.linalg.eig`, which performs eigendecomposition and outputs the eigenvalues and eigenvectors.

The ***eigenvalues*** are related to the relative variation described by each principal component. The ***eigenvectors*** are also known as the principal axes. They tell us how to transform (rotate) our data into new features that capture this variation.

To implement this in Python:

```python
correlation_matrix = data_matrix.corr()
eigenvalues, eigenvectors = np.linalg.eig(correlation_matrix)
```

1. First, we generate a correlation matrix using `.corr()`
2. Next, we use `np.linalg.eig()` to perform eigendecompostition on the correlation matrix. This gives us two outputs — the eigenvalues and eigenvectors.

```python
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import codecademylib3

data_matrix = pd.read_csv('./data_matrix.csv')

# 1. Use the `.corr()` method on `data_matrix` to get the correlation matrix 
correlation_matrix = data_matrix.corr()

## Heatmap code:
red_blue = sns.diverging_palette(220, 20, as_cmap=True)
sns.heatmap(correlation_matrix, vmin = -1, vmax = 1, cmap=red_blue)
plt.show()

# 2. Perform eigendecomposition using `np.linalg.eig` 
eigenvalues, eigenvectors = np.linalg.eig(correlation_matrix)

# 3. Print out the eigenvectors and eigenvalues
print('eigenvectors: ')
print(eigenvectors)

print('eigenvalues: ')
print(eigenvalues)
```

![Untitled](PCA%207395b8d4b2d542f99e6a2aecf4785c10/Untitled%205.png)

## **Implementing PCA in NumPy II - Analysis**

After performing PCA, we generally want to know how useful the new features are. One way to visualize this is to create a scree plot, which shows the proportion of information described by each principal component.

The proportion of information explained is equal to the relative size of each eigenvalue:

```python
info_prop = eigenvalues / eigenvalues.sum()
print(info_prop)
```

To create a scree plot, we can then plot these relative proportions:

```python
plt.plot(np.arange(1,len(info_prop)+1),
         info_prop,
         'bo-')
plt.show()
```

![https://static-assets.codecademy.com/skillpaths/feature-engineering/PCA/scree.svg](https://static-assets.codecademy.com/skillpaths/feature-engineering/PCA/scree.svg)

From this plot, we see that the first principal component explains about 50% of the variation in the data, the second explains about 30%, and so on.

Another way to view this is to see how many principal axes it takes to reach around 95% of the total amount of information. Ideally, we’d like to retain as few features as possible while still reaching this threshold.

To do this, we need to calculate the cumulative sum of the `info_prop` vector we created earlier:

```python
cum_info_prop = np.cumsum(info_prop)
```

We can then plot these values using matplotlib:

```python
plt.plot(np.arange(1,len(info_prop)+1),
         cum_info_prop,
         'bo-')
plt.hlines(y=.95, xmin=0, xmax=15)
plt.vlines(x=4, ymin=0, ymax=1)
plt.show()
```

![https://static-assets.codecademy.com/skillpaths/feature-engineering/PCA/cum_plot.svg](https://static-assets.codecademy.com/skillpaths/feature-engineering/PCA/cum_plot.svg)

From this plot, we see that four principal axes account for 95% of the variation in the data.

```python
import numpy as np
import pandas as pd
import codecademylib3
import matplotlib.pyplot as plt

eigenvalues = pd.read_csv('eigenvalues.csv')['eigenvalues'].values

# 1. Find the proportion of information for each eigenvector, which is equal to the eigenvalues divided by the sum of all eigenvalues
info_prop = eigenvalues / eigenvalues.sum()

## Plot the principal axes vs the information proportions for each principal axis

plt.plot(np.arange(1,len(info_prop)+1),info_prop, 'bo-', linewidth=2)
plt.title('Scree Plot')
plt.xlabel('Principal Axes')
plt.xticks(np.arange(1,len(info_prop)+1))
plt.ylabel('Proportion of Information Explained')
plt.show()
plt.clf()

# 2. Find the cumulative sum of the proportions
cum_info_prop = np.cumsum(info_prop)

## Plot the cumulative proportions array

plt.plot(np.arange(1,len(info_prop)+1), cum_info_prop, 'bo-', linewidth=2)
plt.hlines(y=.95, xmin=1, xmax=15)
plt.vlines(x=4, ymin=0, ymax=1)
plt.title('Cumulative Information Proportions')
plt.xlabel('Principal Axes')
plt.xticks(np.arange(1,len(info_prop)+1))
plt.ylabel('Cumulative Proportion of Information Explained')
plt.show()
```

![Untitled](PCA%207395b8d4b2d542f99e6a2aecf4785c10/Untitled%206.png)

## **Implementing PCA using Scikit-Learn**

Another way to perform PCA is using the scikit-learn module `sklearn.decomposition.PCA`.

The steps to perform PCA using this method are:

- Standardize the data matrix. This is done by subtracting the mean and dividing by the standard deviation of each column vector.

```python
mean = data.mean(axis=0)
sttd = data.std(axis=0)
data_standardized = (data - mean) / sttd
```

- Perform eigendecomposition by fitting the standardized data. We can access the eigenvectors using the `components_` attribute and the proportional sizes of the eigenvalues using the `explained_variance_ratio_` attribute.

```python
pca = PCA()
components = pca.fit(data_standardized).components_
components = pd.DataFrame(components).transpose()
components.index =  data_matrix.columns
print(components)
```

![https://content.codecademy.com/articles/principal-component-analysis-intro/components.png](https://content.codecademy.com/articles/principal-component-analysis-intro/components.png)

```python
var_ratio = pca.explained_variance_ratio_
var_ratio = pd.DataFrame(var_ratio).transpose()
print(var_ratio)
```

![https://content.codecademy.com/articles/principal-component-analysis-intro/prop_var_explained.png](https://content.codecademy.com/articles/principal-component-analysis-intro/prop_var_explained.png)

This module has many advantages over the NumPy method, including a number of different solvers to calculate the principal axes. This can greatly improve the quality of the results.

```python
import numpy as np
import pandas as pd
from sklearn.decomposition import PCA
import codecademylib3

data_matrix = pd.read_csv('./data_matrix.csv')

# 1. Standardize the data matrix
mean = data_matrix.mean(axis=0)
sttd = data_matrix.std(axis=0)
data_matrix_standardized = (data_matrix - mean) / sttd
print(data_matrix_standardized.head())

# 2. Find the principal components
pca = PCA()
components = pca.fit(data_matrix_standardized).components_
components = pd.DataFrame(components).transpose()
components.index =  data_matrix.columns
print(components)

# 3. Calculate the variance/info ratios
var_ratio = pca.explained_variance_ratio_
var_ratio= pd.DataFrame(var_ratio).transpose()
print(var_ratio)
```

## **Projecting the Data onto the principal Axes**

Once we have performed PCA and obtained the eigenvectors, we can use them to project the data onto the first few principal axes. We can do this by taking the dot product of the data and eigenvectors, or by using the `sklearn.decomposition.PCA` module as follows:

```python
from sklearn.decomposition import PCA

# only keep 3 PCs
pca = PCA(n_components = 3)

# transform the data using the first 3 PCs
data_pcomp = pca.fit_transform(data_standardized)

# transform into a dataframe
data_pcomp = pd.DataFrame(data_pcomp)

# rename columns
data_pcomp.columns = ['PC1', 'PC2', 'PC3']

# print the transformed data
print(data_pcomp.head())
```

![https://content.codecademy.com/articles/principal-component-analysis-intro/princomp_head.png](https://content.codecademy.com/articles/principal-component-analysis-intro/princomp_head.png)

Once we have the transformed data, we can look at a scatter plot of the first two transformed features using seaborn or matplotlib. This allows us to view relationships between multiple features at once in 2D or 3D space. Often, the the first 2-3 principal components result in clustering of the data.

Below, we’ve plotted the first two principal components for a dataset of measurements for three different penguin species:

```python
sns.lmplot(x='PC1', y='PC2', data=data_pcomp, hue='species', fit_reg=False)
plt.show()
```

![https://static-assets.codecademy.com/skillpaths/feature-engineering/PCA/pc1pc2.svg](https://static-assets.codecademy.com/skillpaths/feature-engineering/PCA/pc1pc2.svg)

```python
import numpy as np
import pandas as pd
from sklearn.decomposition import PCA
import codecademylib3
import matplotlib.pyplot as plt
import seaborn as sns

data_matrix_standardized = pd.read_csv('./data_matrix_standardized.csv')
classes = pd.read_csv('./classes.csv')['Class']

# 1. Transform the data into 4 new features using the first PCs
pca = PCA(n_components = 4)
data_pcomp = pca.fit_transform(data_matrix_standardized)
data_pcomp = pd.DataFrame(data_pcomp)
data_pcomp.columns = ['PC1', 'PC2', 'PC3', 'PC4']
print(data_pcomp.head())

## 2. Plot the first two principal components colored by the bean classes

data_pcomp['bean_classes'] = classes
sns.lmplot(x='PC1', y='PC2', data=data_pcomp, hue='bean_classes', fit_reg=False)
plt.show()
```

![Untitled](PCA%207395b8d4b2d542f99e6a2aecf4785c10/Untitled%207.png)

## **PCA as Features**

So far we have used PCA to find principal axes and project the data onto them. We can use a subset of the projected data for modeling, while retaining most of the information in the original (and higher-dimensional) dataset.

For example, recall in the previous exercise that the first four principal axes already contained 95% of the total amount of variance (or information) in the original data. We can use the first four components to train a model, just like we would on the original 16 features.

Because of the lower dimensionality, we should expect training times to be faster. Furthermore, the principal axes ensure that each new feature has no correlation with any other, which can result in better model performance.

In this checkpoint, we will be using the first four principal components as our training data for a Support Vector Classifier (SVC). We will compare this to a model fit with the entire dataset (16 features) using the average likelihood score. Average likelihood is a model evaluation metric; the higher the average likelihood, the better the fit.

```python
import pandas as pd
from sklearn.decomposition import PCA
from sklearn.svm import LinearSVC
from sklearn.model_selection import train_test_split
 
 
data_matrix_standardized = pd.read_csv('./data_matrix_standardized.csv')
classes = pd.read_csv('./classes.csv')
 
# We will use the classes as y
y = classes.Class.astype('category').cat.codes
 
# Get principal components with 4 features and save as X
pca_1 = PCA(n_components=4) 
X = pca_1.fit_transform(data_matrix_standardized) 
 
# Split the data into 33% testing and the rest training
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)
 
# Create a Linear Support Vector Classifier
svc_1 = LinearSVC(random_state=0, tol=1e-5)
svc_1.fit(X_train, y_train) 
 
# Generate a score for the testing data
score_1 = svc_1.score(X_test, y_test)
print(f'Score for model with 4 PCA features: {score_1}')
 
# Split the original data intro 33% testing and the rest training
X_train, X_test, y_train, y_test = train_test_split(data_matrix_standardized, y, test_size=0.33, random_state=42)
 
# Create a Linear Support Vector Classifier
svc_2 = LinearSVC(random_state=0)
svc_2.fit(X_train, y_train)
 
# Generate a score for the testing data
score_2 = svc_2.score(X_test, y_test)
print(f'Score for model with original features: {score_2}')
```

## **PCA for Images I**

Another way to show the inner workings of PCA is to use an image dataset. An image can be represented as a row in a data matrix, where each feature corresponds to the intensity of a pixel.

In this and the following exercise, we will be using the [Olivetti Faces](https://scikit-learn.org/stable/datasets/real_world.html?highlight=olivetti+faces) image dataset. We will begin by standardizing the images, and then observing the images of faces themselves.

In the next exercise, we will then transform the original data using PCA and re-plot the images using a subset of the principal components. This will allow us to visualize the mechanism by which PCA retains information in the data while reducing the dimensionality.

```python
import numpy as np
from sklearn import datasets
import matplotlib.pyplot as plt

# Download the data from sklearn's datasets
faces = datasets.fetch_olivetti_faces()['data']

# 1. Standardize the images using the mean and standard deviation
faces_mean = faces.mean(axis=0)
faces_std = faces.std(axis=0)
faces_standardized = (faces - faces_mean) / faces_std

# 2. Find the number of features per image
n_images, n_features = faces_standardized.shape
side_length = int(np.sqrt(n_features))
print(f'Number of features(pixels) per image: {n_features}')
print(f'Square image side length: {side_length}')

# 3. Plot the images
# Create an empty 10x8 plot
fig = plt.figure(figsize=(10, 8))

# Observe the first 15 images.
for i in range(15):

    # Create subplot, remove x and y ticks, and add title
    ax = fig.add_subplot(3, 5, i + 1, xticks=[], yticks=[])
    ax.set_title(f'Image of Face: #{i}')

    # Get an image from a row based on the current value of i
    face_image = faces_standardized[i]

    # Reshape this image into side_length x side_length 
    face_image_reshaped = face_image.reshape(side_length, side_length)

    # Show the image
    ax.imshow(face_image_reshaped, cmap=plt.cm.bone)
plt.show()
```

## **PCA for Images II**

Now that we have cleaned up the data, we can perform PCA to retrieve the eigenvalues and eigenvectors.

This can be useful in understanding how PCA works! We can visualize the eigenvectors by plotting them. They actually have a name: ***eigenfaces***. The eigenfaces are the building blocks for all the other faces in the data.

We can also visualize the dimensionality reduction that occurs when we transform the original data using a smaller number of principal components. In the code editor, we’ve provided you with code to:

- Plot the eigenfaces
- Plot the reconstructed faces using a smaller number of transformed features. To start, we’ve used 400 principal components — only 0.9% of the original number of features (pixels)!

```python
import numpy as np
import pandas as pd
from sklearn.decomposition import PCA
import codecademylib3
import matplotlib.pyplot as plt

faces_standardized = pd.read_csv('./faces_standardized.csv').values

# 1. Instantiate a PCA object and fit the standardized faces dataset
pca = PCA(n_components=40) 
pca.fit(faces_standardized)

# 2. Retrieve and plot eigenvectors (eigenfaces)
eigenfaces = pca.components_ 

fig = plt.figure(figsize=(10, 8))
fig.suptitle('Eigenvectors of Images (Eigenfaces)')
for i in range(15):
    # Create subplot, remove x and y ticks, and add title
    ax = fig.add_subplot(3, 5, i + 1, xticks=[], yticks=[])
    ax.set_title(f'Eigenface: #{i}')
    
    # Get an eigenvector from the current value of i
    eigenface = eigenfaces[i]

    # Reshape this image into 64x64 since the flattened shape was 4096
    eigenface_reshaped = eigenface.reshape(64, 64)

    # Show the image
    ax.imshow(eigenface_reshaped, cmap=plt.cm.bone)
plt.show()

# 3. Reconstruct images from the compressed principal components
# The principal components are usually calculated using `faces_standardized @ principal_axes` or the `.transform` method
principal_components = pca.transform(faces_standardized) 

# The `inverse_transform` method allows for reconstruction of images in the original size
faces_reconstructed = pca.inverse_transform(principal_components)

# Plot the reconstructed images 
fig = plt.figure(figsize=(10, 8))
fig.suptitle('Reconstructed Images from Principal Components')
for i in range(15):
    ax = fig.add_subplot(3, 5, i + 1, xticks=[], yticks=[])
    ax.set_title(f'Reconstructed: {i}')

    reconstructed_face = faces_reconstructed[i]
    reconstructed_face_reshaped = reconstructed_face.reshape(64, 64)
    ax.imshow(reconstructed_face_reshaped, cmap=plt.cm.bone)
plt.show()
```

![Untitled](PCA%207395b8d4b2d542f99e6a2aecf4785c10/Untitled%208.png)

![Untitled](PCA%207395b8d4b2d542f99e6a2aecf4785c10/Untitled%209.png)

## **Exercise # 9: Review**

In this lesson, we have seen how PCA can be implemented using NumPy and scikit-learn. In particular, we have seen how:

- Implementation: scikit-learn provides a more in-depth set of methods and attributes that extend the number of ways to perform PCA or display the percentage of variance for each principal axis.
- Dimensionality reduction: We visualized the data projected onto the principal axes, known as principal components.
- Image classification: We performed PCA on images of faces to visually understand how principal components still retain nearly all the information in the original dataset.
- Improved algorithmic speed/accuracy: Using principal components as input to a classifier, we observed how we could achieve equal or better results with lower dimensional data. Having lower dimensionality also speeds the training.

![Untitled](PCA%207395b8d4b2d542f99e6a2aecf4785c10/Untitled%2010.png)